# Local AI Detection
### Why?
Every current mainstream AI detector is proprietary. This is no fault of their own, an AI detector which has an open method of detecting AI would be trivially easy to bypass. The only problem is that using these tools will inevitably expose whatever you are writing to the broader internet, and thus comes with its own security risks.


Also consider this project (in its current state) to be a test of how good R1 is. Most of the code was written by it, albeit requiring several minor fixes.

### Is it accurate?
No.


Fundamentally AI detectors impossible to make because you do not know the original prompt used. If you know the original prompt, you already know it is AI, so detecting it is useless. In addition, [research](https://github.com/BaleChen/nlu-final-project/blob/main/NLU-final-paper.pdf) shows you need to use the original model as a "judge" to accurately determine if a text is AI. If the detector uses Llama but the text was written with Claude, the detector won't work. 


The purpose of this tool isn't to determine whether a text was written by AI. Instead, it is meant to show you which parts of your writing could potentially be falsely (or truthfully) flagged as AI generated.

### How does it work?
It uses a locally running LLM to generate determine the probability that each token in the input was generated by that model.


### How do I use it?
First, download [llama.cpp](https://github.com/ggerganov/llama.cpp). Compile and run llama-server with a BASE model (NOT a chat model). In the future, when full analysis mode is added, you'll be able to use a non-reasoning (no r-1) chat model.


Next, navigate to your web browser and open up index.html.


Paste in whatever text you want to analyze and hit "run analysis". Once it is finished, the top output box will have each word highlighted. Green or yellow is good, meaning that the LLM didn't think of that particular token as being particularly likely to have been generated. Red means that the AI agrees with what you wrote, which is bad. The lower output box shows raw token probabilities.


### Todo:
Add a "full analysis" mode that first sends the input to another LLM, tells that LLM to generate a prompt that would output the input text, and then analyze the generated prompt + input using the existing method


Add an overall "percent AI score"
