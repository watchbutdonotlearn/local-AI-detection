# Local AI Detection
### Why?
Every current mainstream AI detector is proprietary. Currently, there is no open source AI detector with results similar to mainstream ones.


Also consider this project (in its current state) to be a test of how good R1 is. Most of the code was written by it, albeit requiring several minor fixes.

### Is it accurate?
No, but neither is any other AI detector. Don't go around accusing anyone of using AI just because this tool or any other tool told you so.


Fundamentally AI detectors are impossible to make because you do not know the original prompt used. If you know the original prompt, you already know it is AI, so detecting it is useless. This project uses an LLM to "reverse" the prompt, and appends it onto the beginning of the analysis. In addition, [research](https://github.com/BaleChen/nlu-final-project/blob/main/NLU-final-paper.pdf) shows you need to use the original model as a "judge" to accurately determine if a text is AI. If the detector uses Llama but the text was written with Claude, the detector won't work. 


### How does it work?
It uses a locally running LLM to generate determine the probability that each token in the input was generated by that model. To solve the problem that the detector doesn't know the original prompt used to generate the text, it uses an LLM to "reverse" the original prompt. This isn't always accurate, but it works well enough in my testing. Then, it calculates a few metrics, such as how many tokens the LLM determined were high probability, allowing you to determine if the text was written by AI.


### How do I use it?
First, download [llama.cpp](https://github.com/ggerganov/llama.cpp). Compile and run llama-server with a non-reasoning (not r-1) chat model with the flag `--path /path/to/index.html`. The default is gemma-v2. If you want to use a different model, change the chat template in the settings to the chat template of the model you are using.


Next, navigate to your web browser and open up 127.0.0.1:8080.


Paste in whatever text you want to analyze and hit "run analysis". Once it is finished, the top output box will have each word highlighted. Green or yellow is good, meaning that the LLM didn't think of that particular token as being particularly likely to have been generated. Red means that the AI agrees with what you wrote, which is bad. The bottom box will output various metrics of how "AI" the text is. Usually the best metric to look at is "number of tokens with 0 suspiciousness." If this metric is above 30%, usually the text is not AI.


The detector currently ONLY works on English. It does NOT work for code. Ideally, the text sample should be above 300 words in length, and the generated prompt should at least reasonably fit the input text. If it doesn't, you can change the prompt used to generate the reverse prompt in settings, and try again.


### Todo:
Add new AI detection metrics


Make it easier to install


Add documentation for what different metrics and settings mean


Make the UI look better and move some settings into a more convenient location


### Credits:
[GLTR](https://arxiv.org/abs/1906.04043) provided the basic inspiration for the approach to this project. Consider this project to be a more modern implementation of GLTR with some more features added on. 


Also shoutout to [Sam Peach](https://github.com/sam-paech/antislop-sampler) for providing the "slop" words list.


